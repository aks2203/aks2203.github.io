<html>
 <head>
  <title>Avi Schwarzschild</title>
  <link rel="stylesheet" href="style.css"/>
 </head>
 <body>
  <div style="font-size:36;color:#3159a9";>
    <img src="profile.png" style="width:100px;height:100px;margin-bottom:-1.0em;border-radius:50%;"> <b>Avi Schwarzschild</b>
  </div>
  <br>
  <!-- <h1>Avi Schwarzschild</h1> -->
  <h5><em>Trying to learn about deep learning faster than deep learning can learn about me.</em></h5>
  <p>avi1@umd.edu, [<a href="https://scholar.google.com/citations?user=WNvQ7AcAAAAJ&hl=en&authuser=1">Google Scholar</a>] [<a href="https://twitter.com/A_v_i__S">Twitter</a>] [<a href="https://github.com/aks2203">GitHub</a>] [<a href="cv.pdf">CV</a>]</p>
  <p>I am a fifth-year Ph.D. student in the Applied Math and Scientific Computation program at the University of Maryland. I am advised by <a href="https://www.cs.umd.edu/~tomg/">Tom Goldstein</a> on my work in deep learning. My general interests range from security to generalization and my work focuses on exapanding our understanding of when and why neural networks work. My specific interest in data security and model vulnerability has led to work on adversarial attacks and data poisoning. I am also investigating neural networks' ability to extrapolate from easy training tasks to more difficult problems at test time. I am in the market for a post-doc position beginning in the Fall of 2023.</p>

  <p>For the summer of 2022, I was a researcher at <a href="https://www.arthur.ai">Arthur AI</a> in New York City. I am currently continuing my work there on consistency of post-hoc explainers as a part-time intern.</p>

  <p>Before starting at UMD, I received a master's degree in applied math at the University of Washington and a bachelor's degree in applied math at Columbia Engineering.</p>
<hr color="#7e98cc" size="4">
  <h2>Work Under Review</h2>
  <p class="pub"> <b>Avi Schwarzschild*</b>, Alex Stein*, Michael Curry, Tom Goldstein, and John Dickerson. <u>Protecting Bidder Information in Neural Auctions</u>. <i>Preprint</i>. [<a href="https://openreview.net/pdf?id=b5RD94lXu2j">OpenReview</a>]</p>
  <p class="pub">Roman Levin, Valeriia Cherepanova, <b>Avi Schwarzschild</b>, Arpit Bansal, C Bayan Bruss, Tom Goldstein, Andrew Gordon Wilson, and Micah Goldblum. <u>Transfer Learning with Deep Tabular Models</u>. <i>Under Review</i>. [<a href="https://arxiv.org/pdf/2206.15306.pdf">ArXiv</a>]</p>
  <p class="pub">Gowthami Somepalli, Micah Goldblum, <b>Avi Schwarzschild</b>, C Bayan Bruss, and Tom Goldstein. <u>SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training</u>. <i>Under Review</i>. [<a href="https://arxiv.org/pdf/2106.01342.pdf">ArXiv</a>]</p>
  <p class="pub">Arpit Bansal, Micah Goldblum, Valeriia Cherepanova, <b>Avi Schwarzschild</b>, C Bayan Bruss, and Tom Goldstein. <u>MetaBalance: High-Performance Neural Networks for Class-Imbalanced Data</u>. <i>Under Review</i>. [<a href="https://arxiv.org/pdf/2106.09643.pdf">ArXiv</a>]</p>  
<hr color="#7e98cc" size="4">
  <h2>Published</h2>
  <p class="pub">Arpit Bansal*, <b>Avi Schwarzschild*</b>, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah  Goldblum, and Tom Goldstein. <u>End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking</u>. <i>Neural Information Processing Systems</i> (<b>NeurIPS</b>), 2022. [<a href="https://arxiv.org/pdf/2202.05826.pdf">ArXiv</a>]</p>
  <p class="pub"> Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, <b>Avi Schwarzschild</b>, Dawn Song, Aleksander Madry, Bo Li, and Tom Goldstein. <u>Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses</u>. <i>IEEE transactions on pattern analysis and machine intelligence</i> (<b>TPAMI</b>), 2022. [<a href="https://arxiv.org/pdf/2012.10544.pdf">ArXiv</a>][<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9743317">Published Version</a>]</p> 
  <p class="pub"> <b>Avi Schwarzschild*</b>, Arjun Gupta*, Amin Ghiasi, Micah Goldblum, and Tom Goldstein. <u>The Uncanny Similarity of Recurrence and Depth</u>. <i>International Conference on Learning Representations</i> (<b>ICLR</b>), 2022. [<a href="https://openreview.net/pdf?id=3wNcr5nq56">Published Version</a>]</p> 
  <p class="pub"> <b>Avi Schwarzschild</b>, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. <u>Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks</u>. <i>Neural Information Processing Systems</i> (<b>NeurIPS</b>), 2021. [<a href="https://arxiv.org/pdf/2106.04537.pdf">Published Version</a>]</p> 
  <p class="pub"> Micah Goldblum*, <b>Avi Schwarzschild*</b>, Ankit Patel, and Tom Goldstein. <u>Adversarial Attacks on Machine Learning Systems for High-Frequency Trading</u>. <i>International Conference on AI in Finance</i> (<b>ICAIF</b>), 2021. [<a href="https://arxiv.org/pdf/2002.09565.pdf">Published Version</a>]</p> 
  <p class="pub"> <b>Avi Schwarzschild*</b>, Micah Goldblum*, Arjun Gupta, John Dickerson, and Tom Goldstein. <u>Just How Toxic is Data Poisonig? A Benchmark for Backdoor and Data Poisoning Attacks</u>. <i>International Conference on Machine Learning</i> (<b>ICML</b>), 2021. [<a href="https://arxiv.org/pdf/2006.12557.pdf">Published Version</a>]</p> 
  <p class="pub"> Ahmed Abdelkader, Michael Curry, Liam Fowl, Tom Goldstein, <b>Avi Schwarzschild</b>, Manli Shu, Cristoph Studer, and Chen Zhu. <u>Headless Horseman: Adversarial Attacks on Transfer Learning Models</u>. <i>IEEE International Conference on Acoustics, Speech, and Signal Processing</i> (<b>ICASSP</b>), 2020. [<a href="https://arxiv.org/pdf/2004.09007.pdf">Published Version</a>]</p> 
  <p class="pub"> Micah Goldblum*, Jonas Geiping*, <b>Avi Schwarzschild</b>, Michael Moeller, and Tom Goldstein. <u>Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory</u>. <i>International Conference on Learning Representations</i> (<b>ICLR</b>), 2020. [<a href="https://openreview.net/pdf?id=HyxyIgHFvr">Published Version</a>]</p>

  <div class="banner"><em>Last updated November 2022.</em></div>
 </body>
</html>














